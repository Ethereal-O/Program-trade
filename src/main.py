from dispatcher.td3_trainer import TD3Trainer
from dispatcher.reporter import Reporter
from configs import configs


# Select: we use Strategy to select the data we need
# In this project, we first use MODE to control if need to train or report
# FIRST OF ALL, we need to generate the data we need
# to avoid train so many stocks in one time, and in fact we don't need so many stocks
# we first select SELECTED_DATA_NUM stocks from the data, which is defined in configs.py
# For now, we just select them by rsi and adx index, because they are easy to calculate and are enough for us
# To avoid some stocks which is newer for us, we also set a MIN_DATA_ACCECPT_NUM, which is defined in configs.py
# when we select data, we avoid the stocks which has less than MIN_DATA_ACCECPT_NUM data, beacuse they have not enough data ,both for a normal buyer to buy stock in fact or for us to train the model
# as we all known, rsi is a index to show if the stock is overbought or oversold, and adx is a index to show if the stock is in a trend
# the more rsi index are, the more oversold the stock is, and the more adx index are, the more trend the stock is
# so when rsi index is high and adx index is high, we think the stock is oversold and in a trend, so we prefer to buy it
# NOTE: The select is automatic, when we run train or report, we both need it, so they will first check and generate the data they need

# Train: we use TD3Trainer to train the model
# when the MODE is MODE_TRAIN, we will train the model
# the data is splited into two parts, the first part is used to train the model, and the second part is used to test the model
# we define the TRAIN_TEST_SPLIT in configs.py, show where we need to split the data
# TD3 is a model which use actor-critic to train the model, and it is a off-policy model, so we use replay buffer to store the data
# to generate a env for it, we write a file named env.py, which is in ml folder
# it can be generated by the data we give it, and it can also be generated by default.
# the reason of supplying the data to generate, is that the reading data is slow, so we can generate the env first, and then use it to train the model
# and for eval env, we can just supply the same data to it. Because the data is READ-ONLY, so it can have much faster speed by the COW mechanism of OS
# For more speed, we can use thread to generate the env, and use multiprocessing to train the model(which is not implemented now)
# Because the num of the data is just about 1e8, so we don't need much time to train the model, so we just use one thread to generate the env and train the model
# TD3Trainer will train the model and save it to the model path, which is defined in configs.py
# the update of actor and critic is update after a start num of steps, which is defined in configs.py, which is called START_TIMESTEPS
# after this, it can select data from the replay buffer to replay and train the model EVERY STEP
# the update rate of the target actor and critic is defined in configs.py, which is called policy_freq
# the update percentage of the target actor and critic is defined in configs.py, which is called tau
# the network of actor is just a simple network, which has 3 layers, and the network of critic is also a simple network, which has 4 layers
# but, it is important to note that the output of the actor is tanh, so we must not make the parameters of the last layer of the actor too large, or the output will be around 1 or -1, which is not what we want
# so we set DATA_SCALE in configs.py, which is used to scale the data, and we also set the max_action in TD3, which is used to scale the output of the actor
# For more information about TD3, you can read the paper: https://arxiv.org/abs/1802.09477

# Report: we use Reporter to report the result
# here we use the model we trained to test the data we have
# we use the the split point we defined in configs.py to split the data
# we use the data before the split point to get markowitz index, and use the data after the split point to test the model
# the markowitz index is a index to show how good the stock is, and it is defined in indexes.py
# so the more markowitz index is, the better the stock is
# we use the weight caculated by markowitz index to decide how much money we need to put in the stock, and we use the model to decide how much to buy and sell the stock in a single time
# the final money is caculated by the money we have and the money we get from the stock

# How to use:
# we can change the configs in configs.py to change the behavior of the program
# also can set some arguments to change the behavior of the program
# the arguments is defined in parser.py
# for example, we can use --force_select_data=True to force to generate selected data

def main():
    if configs.MODE == configs.MODE_TRAIN:
        TD3Trainer.start()
    if configs.MODE == configs.MODE_REPORT:
        Reporter.start()


if __name__ == "__main__":
    main()
